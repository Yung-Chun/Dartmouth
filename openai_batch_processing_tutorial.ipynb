{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APIConnectionError, APITimeoutError, InternalServerError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d2e5c",
   "metadata": {},
   "source": [
    "# API keys\n",
    "Easily manage your API keys by setting up an .env file and updating it programmatically. This prevents data leaks and enhances security by keeping sensitive information out of your code. Always set these keys as environment variables and restart the kernel after updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b34bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_update_token(env_file, token_name, token_value):\n",
    "    # Read or create an existing .env file\n",
    "    env_file_path = Path(env_file)\n",
    "    if not env_file_path.exists():\n",
    "        print(f\"{env_file} does not exist.\")\n",
    "        # create an .env file\n",
    "        env_file_path.touch()\n",
    "        print(f\".env file created at {env_file_path}\")\n",
    "    \n",
    "    # Read lines from the .env file\n",
    "    with open(env_file, 'r') as file:\n",
    "        print(f\"{env_file} exists.\")\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Track if the token was updated\n",
    "    token_exists = False\n",
    "    \n",
    "    # Modify the existing token if it exists\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(f\"{token_name}=\"):\n",
    "            print(f\"{token_name} already exists.\")\n",
    "            lines[i] = f\"{token_name}={token_value}\\n\"\n",
    "            token_exists = True\n",
    "            break\n",
    "    \n",
    "    # If the token does not exist, append it\n",
    "    if not token_exists:\n",
    "        print(f\"Add {token_name}.\")\n",
    "        lines.append(f\"{token_name}={token_value}\\n\")\n",
    "    \n",
    "    # Write the lines back to the .env file\n",
    "    with open(env_file, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "    \n",
    "    print(f\"Token {token_name} has been {'updated' if token_exists else 'added'} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store keys\n",
    "add_or_update_token('your .env file path', 'OPENAI_API_KEY', 'your API key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keys\n",
    "folder_path = '../APIS'\n",
    "env_file_path = os.path.join(folder_path, '.env')\n",
    "\n",
    "with open(env_file_path, 'r') as file:\n",
    "    # print(file.read())\n",
    "    print('.env file exists')\n",
    "\n",
    "load_dotenv(dotenv_path = env_file_path)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# openai_org_id = os.getenv('OPENAI_ORG_ID') # add if needed\n",
    "\n",
    "# # Set as environment variables\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "# os.environ['OPENAI_ORG_ID'] = openai_org_id # add if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da1043",
   "metadata": {},
   "source": [
    "# Load and preprocess your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('your raw data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3002e7",
   "metadata": {},
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Choose your model\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27068f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example\n",
    "\n",
    "label_system_prompt = '''\n",
    "As a political researcher analyzing U.S. elections, your goal is to analyze the topics in the provided post.\n",
    "Please provide the probability that the given text refers to a specific topic, with 1% indicating very unlikely and 99% indicating extremely likely.\n",
    "You will need to output a JSON object.\n",
    "\n",
    "{\n",
    "    \"election\": int,    // The probability that the text references the 2024 Presidential Election.\n",
    "    \"protest\": int,     // The probability that the text references some form of protest.\n",
    "    \"nation\": int,      // The probability that the text references country/countries.\n",
    "    \"technology\": int,  // The probability that the text references technology.\n",
    "    \"religion\": int,    // The probability that the text references religious themes or religious symbols.\n",
    "    \"war\": int,         // The probability that the text references ongoing wars such as Israel/Gaza, Russia/Ukraine, or China/Taiwan.\n",
    "    \"politicians\": int, // The probability that the text references political rallies or specific politicians.\n",
    "    \"energy\": int,      // The probability that the text references climate change or energy issues.\n",
    "    \"economy\": int,     // The probability that the text references economic issues.\n",
    "    \"immigration\": int, // The probability that the text references immigration.\n",
    "    \"healthcare\": int,  // The probability that the text references healthcare.\n",
    "    \"abortion\": int,    // The probability that the text references abortion.\n",
    "    \"race\": int         // The probability that the text references issues of race.\n",
    "    \"others\": int       // The probability that the text references topics not listed above.\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5285bc8",
   "metadata": {},
   "source": [
    "# Create tasks\n",
    "- Ensure `custom_id` is unique, and no null values exist in `custom_id` or `messages`. \n",
    "- The output must be saved as an object in a `.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89936bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTask(row):\n",
    "    return {\n",
    "        \"custom_id\": str(row.id), # must ensure the id is string and unique\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": model_name,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 150,\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_object\"\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": label_system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": row.text # this is the text you want to analyze\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "\n",
    "for idx, row in tqdm(data.iterrows(), total=data.shape[0], desc='Creating tasks'):\n",
    "    task = createTask(row)\n",
    "    tasks.append(task)\n",
    "\n",
    "print(f'Number of tasks: {len(tasks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tasks\n",
    "file_name = 'your_batch_tasks.jsonl'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    for obj in tasks:\n",
    "        f.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759a0ed",
   "metadata": {},
   "source": [
    "# Batch Functions for OpenAI API\n",
    "\n",
    "1. **`writeBatchFile()`**  \n",
    "   Splits tasks into multiple smaller files to avoid hitting API rate limits. Each file contains a manageable number of tasks and is saved as `.jsonl`.\n",
    "\n",
    "\n",
    "2. **`uploadBatchFile()`**  \n",
    "   Uploads the batch files to the OpenAI batch API.\n",
    "\n",
    "\n",
    "3. **`createBatchJob()`**  \n",
    "   Initiates a batch job on the OpenAI batch API using the uploaded batch files.\n",
    "\n",
    "\n",
    "4. **`checkLastBatchTask()`**  \n",
    "   Verifies the last processed batch file ID and continues indexing from where it left off, avoiding duplicate processing or missed tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeBatchFile(batch_data, file_name, directory='batch_tasks'):\n",
    "    # Create the directory if it doesn't exist\n",
    "    path = Path(directory)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = path / file_name\n",
    "\n",
    "    try:\n",
    "        with file_path.open('w') as f:\n",
    "            for obj in batch_data:\n",
    "                f.write(json.dumps(obj) + '\\n')\n",
    "        print(f'File {file_path} created with {len(batch_data)} requests.', end='\\n\\n')\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload batch file\n",
    "def uploadBatchFile(file_name, directory='batch_tasks'):\n",
    "    file_path = Path(directory) / file_name\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File {file_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    print('Uploading batch file...')\n",
    "\n",
    "    try:\n",
    "        with file_path.open('rb') as file:\n",
    "            batch_file = client.files.create(\n",
    "                file=file,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "        print('Batch file name:', batch_file.filename)\n",
    "        print('Batch file ID:', batch_file.id)\n",
    "        print('Batch file status:', batch_file.status, end='\\n\\n')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    return batch_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch job\n",
    "# Do not change endpoint and completion_window here untill OpenAI provides more choices\n",
    "def createBatchJob(batch_file, endpoint=\"/v1/chat/completions\", completion_window=\"24h\"): \n",
    "    print('Creating batch job...')\n",
    "    try:\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=batch_file.id,\n",
    "            endpoint=endpoint,\n",
    "            completion_window=completion_window\n",
    "        )\n",
    "        print('Batch job ID:', batch_job.id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch job: {e}\")\n",
    "        return None\n",
    "\n",
    "    return batch_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee53a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBatchJobStatus(batch_job_id, check_interval=3): #unit of check_interval: minute\n",
    "    batch_job = client.batches.retrieve(batch_job_id)\n",
    "    final_statuses = {'completed', 'failed', 'expired', 'cancelled'}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            batch_job = client.batches.retrieve(batch_job_id)\n",
    "            current_status = batch_job.status.lower()\n",
    "            \n",
    "            print(f\"Current status of job {batch_job_id}: {current_status}\")\n",
    "            \n",
    "            if current_status in final_statuses:\n",
    "                print(f\"Job {batch_job_id} has reached a final status: {current_status}\")\n",
    "                return current_status\n",
    "            \n",
    "            if current_status == 'finalizing':\n",
    "                print(f\"Job {batch_job_id} is finalizing. Checking again in 2 minutes...\")\n",
    "                time.sleep(2 * 60)\n",
    "\n",
    "            else:\n",
    "                print(f\"Job {batch_job_id} is still {current_status}. Checking again in {check_interval} minutes...\")\n",
    "                time.sleep(check_interval * 60)  # Convert minutes to seconds\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while checking job {batch_job_id}: {str(e)}\")\n",
    "            print(f\"Retrying in {check_interval} minutes...\")\n",
    "            time.sleep(check_interval * 60)\n",
    "\n",
    "        print('===========================', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLastBatchTask():\n",
    "   # check numbers of previous batch jobs\n",
    "    pre_batch_jobs = glob('batch_tasks/*.jsonl')\n",
    "    print(f'Number of previous batch jobs: {len(pre_batch_jobs)}')\n",
    "\n",
    "    # Define the directory containing the files\n",
    "    directory = 'batch_tasks'\n",
    "\n",
    "    # Initialize a variable to store the largest ID\n",
    "    largest_id = -1\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Use a regular expression to match the pattern and extract the ID\n",
    "        match = re.search(r'your task name(\\d+)\\.jsonl$', filename)\n",
    "        \n",
    "        if match:\n",
    "            # Convert the extracted ID to an integer\n",
    "            file_id = int(match.group(1))\n",
    "            \n",
    "            # Update the largest ID if the current one is greater\n",
    "            if file_id > largest_id:\n",
    "                largest_id = file_id\n",
    "\n",
    "    print(f\"The largest ID found is: {largest_id}\")\n",
    "    return largest_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96571828",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_id = check_last_batch_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of requests\n",
    "batch_size = 20000 # adjust the size here according to your permit level\n",
    "num_files = (len(tasks) + batch_size - 1) // batch_size\n",
    "print(f'Total requests: {len(tasks)}. Batch size: {batch_size}. Separated in {num_files} files.', end='\\n\\n')\n",
    "\n",
    "# batch_job_records = []\n",
    "\n",
    "# Save data in chunks\n",
    "for i in range(num_files):\n",
    "    start_index = i * batch_size\n",
    "    end_index = start_index + batch_size\n",
    "    \n",
    "    # Slice the list to get the current batch\n",
    "    batch_data = tasks[start_index:end_index]\n",
    "    \n",
    "    # Create a filename\n",
    "    file_name = f'your task name{i+1+largest_id}.jsonl'\n",
    "\n",
    "    # record the batch job if required\n",
    "#     batch_job_records.append({\n",
    "#         'file_name': file_name,\n",
    "#         'start_index': start_index,\n",
    "#         'end_index': end_index\n",
    "#     })\n",
    "    \n",
    "    # Write batch tasks\n",
    "    writeBatchFile(batch_data, file_name)\n",
    "\n",
    "    # Upload batch file\n",
    "    batch_file = uploadBatchFile(file_name)\n",
    "    if not batch_file:\n",
    "        print(f\"Failed to upload file {file_name}. Skipping this batch.\")\n",
    "        break\n",
    "        # continue\n",
    "    \n",
    "    # Create batch job\n",
    "    batch_job = createBatchJob(batch_file)\n",
    "    if not batch_job:\n",
    "        print(f\"Failed to create batch job for file {file_name}.\")\n",
    "        print(batch_job.errors.data)\n",
    "        continue\n",
    "    \n",
    "    # Check batch job status, until it reaches a final status\n",
    "    final_statuses = {'completed', 'failed', 'expired', 'cancelled'}\n",
    "    final_status = checkBatchJobStatus(batch_job.id)\n",
    "    if final_status in final_statuses:\n",
    "        print(f\"Batch job {batch_job.id} is {final_status}.\")\n",
    "        continue\n",
    "\n",
    "print(\"Batch processing completed.\")\n",
    "\n",
    "# save batch job records as a jsonl file\n",
    "# with open('batch_job_records.jsonl', 'w') as f:\n",
    "#     for obj in batch_job_records:\n",
    "#         f.write(json.dumps(obj) + '\\n')\n",
    "# print('Batch job records saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27c091",
   "metadata": {},
   "source": [
    "# Retrieve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c741a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'batch_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLastBatchOutput(output_directory):\n",
    "\n",
    "    # Check numbers of previous batch output\n",
    "    pre_batch_jobs = glob(f'{output_directory}/*.json')\n",
    "    print(f'Number of previous batch output: {len(pre_batch_jobs)}')\n",
    "\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        print(f\"Directory {output_directory} does not exist.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize a variable to store the largest ID\n",
    "    largest_id = -1\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(output_directory):\n",
    "        match = re.search(r'your task result(\\d+)\\.json$', filename)\n",
    "        if match:\n",
    "            file_id = int(match.group(1))\n",
    "            if file_id > largest_id:\n",
    "                largest_id = file_id\n",
    "          \n",
    "    if largest_id == -1:\n",
    "        print(\"No valid IDs found in the directory.\")\n",
    "    else:\n",
    "        print(f\"The largest ID found is: {largest_id}\")\n",
    "\n",
    "    return largest_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_id = checkLastBatchOutput(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = []\n",
    "\n",
    "# Fetch batches from the client.\n",
    "# Replace 'specific_batch_id' with the ID just above your desired batch ID \n",
    "# (as the OpenAI Batch API dashboard lists batches from latest to previous).\n",
    "# Note: You can use either `after` to specify the starting batch ID \n",
    "# or `limit` to define the maximum number of batches to retrieve, but not both simultaneously.\n",
    "for batch in client.batches.list(after='specific_batch_id', limit=50):  \n",
    "    # Replace 'specific_batch_id' with the batch ID above the target or remove `limit` to use one parameter.\n",
    "\n",
    "#     if batch.id == 'batch_id_to_exclude':  # Skip the batch with the specified ID if needed\n",
    "#         break\n",
    "\n",
    "    print(f\"Batch ID: {batch.id}, Status: {batch.status}\")\n",
    "\n",
    "    # Collect only completed batches, skipping failed or canceled ones\n",
    "    if batch.status == 'completed':\n",
    "        output_files.append([batch.id, batch.created_at, batch.output_file_id])\n",
    "\n",
    "# Print the total number of completed batches retrieved\n",
    "print(f\"Total completed batches: {len(output_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sort by created time\n",
    "output_files = sorted(output_files, key=lambda x: x[1])\n",
    "len(output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ebe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, output_file in enumerate(output_files):\n",
    "    path = Path(output_directory)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    output_file_name = f'your task result{idx+1+largest_id}.json'\n",
    "    # break\n",
    "    file_path = path / output_file_name\n",
    "    print(file_path)\n",
    "\n",
    "    result = client.files.content(output_files[idx][2]).content\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a16c88",
   "metadata": {},
   "source": [
    "# Organize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "directory = 'batch_output'\n",
    "output_files = glob(f'{directory}/*.json')\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_path in tqdm(output_files, total=len(output_files), desc='Processing files'):\n",
    "    # print(f\"Processing file: {file_path}\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON string into a dictionary\n",
    "            json_object = json.loads(line.strip())\n",
    "            \n",
    "            # Extract the custom_id\n",
    "            custom_id = json_object.get('custom_id', None)\n",
    "            \n",
    "            # Extract the response body and parse the content to get values\n",
    "            response_content = json.loads(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "            \n",
    "            # Add the custom_id to the dictionary\n",
    "            response_content['id'] = custom_id\n",
    "            \n",
    "            # Append to results\n",
    "            results.append(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process your results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb92299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
